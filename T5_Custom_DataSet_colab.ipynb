{
 "cells": [
  {
   "source": [
    "#### T5  \n",
    "https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html (https://youtu.be/r6XY80Z9eSA?t=348)  \n",
    "https://huggingface.co/transformers/model_doc/t5.html  \n",
    "https://arxiv.org/abs/1910.10683  \n",
    "\n",
    "\n",
    "https://www.youtube.com/watch?v=_l2wJb3QPdk  \n",
    "https://www.youtube.com/watch?v=r6XY80Z9eSA  \n",
    "\n",
    "pip install --quiet transformers==4.1.1  \n",
    "pip install --quiet pytorch-lightning==1.1.1  \n",
    "pip install --quiet tokenizers==0.9.4  \n",
    "pip install --quiet sentencepiece==0.1.94  Not needed, included in the tokenizers\n",
    "pip install --quiet pandas  \n",
    "pip install --quiet sklearn  \n",
    "pip install --quiet keras  \n",
    "pip install --quiet tensorflow  \n",
    "pip install --quiet termcolor  \n",
    "\n",
    "## Tutorial, Part 1\n",
    "https://www.youtube.com/watch?v=_l2wJb3QPdk\n",
    "\n",
    "In google colab change runtime to GPU"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet transformers==4.1.1\n",
    "!pip install --quiet pytorch-lightning==1.1.1\n",
    "!pip install --quiet tokenizers==0.9.4\n",
    "!pip install --quiet sklearn\n",
    "!pip install --quiet keras\n",
    "!pip install --quiet tensorflow\n",
    "!pip install --quiet termcolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as py\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from termcolor import colored\n",
    "import textwrap\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "\n",
    "# model files are downloaded from https://huggingface.co/valhalla/t5-base-qa-qg-hl/tree/main\n",
    "# if Internet access is available just use\n",
    "# MODEL_FILES = \"t5-base\"\n",
    "# instead of path to model files\n",
    "\n",
    "#from sys import platform\n",
    "#if \"linux\" in platform.lower():\n",
    "#    MODEL_FILES = \"/home/myuser/TransformerModels/t5-base-qa-qg-hl\"\n",
    "#    CHECKPOINT_PATH=\"/home/myuserTransformerModels/_CheckPoints\"\n",
    "#else:\n",
    "#    MODEL_FILES = \"C:/TransformerModels/t5-base-qa-qg-hl\"\n",
    "#    CHECKPOINT_PATH=\"C:/TransformerModels/_CheckPoints\"\n",
    "\n",
    "MODEL_FILES = \"t5-base\"\n",
    "CHECKPOINT_PATH=\"./CheckPoints\"\n",
    "\n",
    "\n",
    "N_GPUS = 1 # Change here if you have GPUs\n",
    "N_WORKERS = 4 # 4 in the tutorial. 0 if running on windows without GPU...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = AutoModelWithLMHead.from_pretrained(\"deep-learning-analytics/triviaqa-t5-base\")\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/dmis-lab/biobert#datasets\n",
    "!gdown --id 19ft5q44W4SuptJgTwR84xZjsHg1jvjSZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q QA.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(\"BioASQ/BioASQ-train-factoid-4b.json\").open() as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'qas': [{'id': '52bf208003868f1b06000019_002',\n",
       "   'question': 'What is the inheritance pattern of Li–Fraumeni syndrome?',\n",
       "   'answers': [{'text': 'autosomal dominant', 'answer_start': 213}]}],\n",
       " 'context': 'Balanced t(11;15)(q23;q15) in a TP53+/+ breast cancer patient from a Li-Fraumeni syndrome family. Li-Fraumeni Syndrome (LFS) is characterized by early-onset carcinogenesis involving multiple tumor types and shows autosomal dominant inheritance. Approximately 70% of LFS cases are due to germline mutations in the TP53 gene on chromosome 17p13.1. Mutations have also been found in the CHEK2 gene on chromosome 22q11, and others have been mapped to chromosome 11q23. While characterizing an LFS family with a documented defect in TP53, we found one family member who developed bilateral breast cancer at age 37 yet was homozygous for wild-type TP53. Her mother also developed early-onset primary bilateral breast cancer, and a sister had unilateral breast cancer and a soft tissue sarcoma. Cytogenetic analysis using fluorescence in situ hybridization of a primary skin fibroblast cell line revealed that the patient had a novel balanced reciprocal translocation between the long arms of chromosomes 11 and 15: t(11;15)(q23;q15). This translocation was not present in a primary skin fibroblast cell line from a brother with neuroblastoma, who was heterozygous for the TP53 mutation. There was no evidence of acute lymphoblastic leukemia in either the patient or her mother, although a nephew did develop leukemia and died in childhood. These data may implicate the region at breakpoint 11q23 and/or 15q15 as playing a significant role in predisposition to breast cancer development.'}"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "questions = data[\"data\"][0][\"paragraphs\"]\n",
    "questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_questions_and_answers(factoid_path: Path):\n",
    "    with factoid_path.open() as json_file:\n",
    "        data = json.load(json_file)\n",
    "        \n",
    "    questions = data[\"data\"][0][\"paragraphs\"]\n",
    "    data_rows = []\n",
    "    for question in questions:\n",
    "        context = question['context']\n",
    "        for question_and_answers in question['qas']:\n",
    "            question = question_and_answers[\"question\"]\n",
    "            answers = question_and_answers[\"answers\"]\n",
    "            \n",
    "        for answer in answers:\n",
    "            answer_text = answer[\"text\"]\n",
    "            answer_start = answer[\"answer_start\"]\n",
    "            answer_end = answer_start + len(answer_text)\n",
    "            \n",
    "            data_rows.append({\n",
    "                \"question\":question,\n",
    "                \"context\":context,\n",
    "                \"answer_text\": answer_text,\n",
    "                \"answer_start\":answer_start,\n",
    "                \"answer_end\":answer_end\n",
    "            })\n",
    "            \n",
    "    return pd.DataFrame(data_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                question  \\\n",
       "0     What is the inheritance pattern of Li–Fraumeni...   \n",
       "1     What is the inheritance pattern of Li–Fraumeni...   \n",
       "2       Which type of lung cancer is afatinib used for?   \n",
       "3     Which hormone abnormalities are characteristic...   \n",
       "4     Which hormone abnormalities are characteristic...   \n",
       "...                                                 ...   \n",
       "3261  Which is the receptor for substrates of Chaper...   \n",
       "3262  Which is the receptor for substrates of Chaper...   \n",
       "3263  Which is the receptor for substrates of Chaper...   \n",
       "3264  Which is the receptor for substrates of Chaper...   \n",
       "3265  How many selenoproteins are encoded in the hum...   \n",
       "\n",
       "                                                context         answer_text  \\\n",
       "0     Balanced t(11;15)(q23;q15) in a TP53+/+ breast...  autosomal dominant   \n",
       "1     Genetic modeling of Li-Fraumeni syndrome in ze...  autosomal dominant   \n",
       "2     Clinical perspective of afatinib in non-small ...   EGFR-mutant NSCLC   \n",
       "3     DOCA sensitive pendrin expression in kidney, h...             thyroid   \n",
       "4     Clinical and molecular characteristics of Pend...             thyroid   \n",
       "...                                                 ...                 ...   \n",
       "3261  Regulation of lamp2a levels in the lysosomal m...              lamp2a   \n",
       "3262  Unique properties of lamp2a compared to other ...              Lamp2a   \n",
       "3263  Unique properties of lamp2a compared to other ...              lamp2a   \n",
       "3264  Dietary lipids and aging compromise chaperone-...              LAMP2A   \n",
       "3265  Characterization of mammalian selenoproteomes....                  25   \n",
       "\n",
       "      answer_start  answer_end  \n",
       "0              213         231  \n",
       "1              105         123  \n",
       "2             1203        1220  \n",
       "3              419         426  \n",
       "4              705         712  \n",
       "...            ...         ...  \n",
       "3261           337         343  \n",
       "3262            62          68  \n",
       "3263          1112        1118  \n",
       "3264           616         622  \n",
       "3265           532         534  \n",
       "\n",
       "[3266 rows x 5 columns]>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "extract_questions_and_answers(Path(\"BioASQ/BioASQ-train-factoid-4b.json\")).head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[WindowsPath('BioASQ/BioASQ-train-factoid-4b.json'),\n",
       " WindowsPath('BioASQ/BioASQ-train-factoid-5b.json'),\n",
       " WindowsPath('BioASQ/BioASQ-train-factoid-6b.json')]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "factoid_paths = sorted(list(Path(\"BioASQ/\").glob(\"BioASQ-train-*\")))\n",
    "factoid_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for factoid_path in factoid_paths:\n",
    "    dfs.append(extract_questions_and_answers(factoid_path))\n",
    "    \n",
    "df = pd.concat(dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "443\n661\n2582\n"
     ]
    }
   ],
   "source": [
    "print(len(df.question.unique()))\n",
    "print(len(df.answer_text.unique()))\n",
    "print(len(df.context.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                            question  \\\n",
       "0  What is the inheritance pattern of Li–Fraumeni...   \n",
       "1  What is the inheritance pattern of Li–Fraumeni...   \n",
       "2    Which type of lung cancer is afatinib used for?   \n",
       "3  Which hormone abnormalities are characteristic...   \n",
       "4  Which hormone abnormalities are characteristic...   \n",
       "\n",
       "                                             context         answer_text  \\\n",
       "0  Balanced t(11;15)(q23;q15) in a TP53+/+ breast...  autosomal dominant   \n",
       "1  Genetic modeling of Li-Fraumeni syndrome in ze...  autosomal dominant   \n",
       "2  Clinical perspective of afatinib in non-small ...   EGFR-mutant NSCLC   \n",
       "3  DOCA sensitive pendrin expression in kidney, h...             thyroid   \n",
       "4  Clinical and molecular characteristics of Pend...             thyroid   \n",
       "\n",
       "   answer_start  answer_end  \n",
       "0           213         231  \n",
       "1           105         123  \n",
       "2          1203        1220  \n",
       "3           419         426  \n",
       "4           705         712  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>context</th>\n      <th>answer_text</th>\n      <th>answer_start</th>\n      <th>answer_end</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What is the inheritance pattern of Li–Fraumeni...</td>\n      <td>Balanced t(11;15)(q23;q15) in a TP53+/+ breast...</td>\n      <td>autosomal dominant</td>\n      <td>213</td>\n      <td>231</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What is the inheritance pattern of Li–Fraumeni...</td>\n      <td>Genetic modeling of Li-Fraumeni syndrome in ze...</td>\n      <td>autosomal dominant</td>\n      <td>105</td>\n      <td>123</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Which type of lung cancer is afatinib used for?</td>\n      <td>Clinical perspective of afatinib in non-small ...</td>\n      <td>EGFR-mutant NSCLC</td>\n      <td>1203</td>\n      <td>1220</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Which hormone abnormalities are characteristic...</td>\n      <td>DOCA sensitive pendrin expression in kidney, h...</td>\n      <td>thyroid</td>\n      <td>419</td>\n      <td>426</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Which hormone abnormalities are characteristic...</td>\n      <td>Clinical and molecular characteristics of Pend...</td>\n      <td>thyroid</td>\n      <td>705</td>\n      <td>712</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "source": [
    "## The duplicates removal in the following cells is done and explained in the second video"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(12988, 5)\n443\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(len(df.question.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=[\"context\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2582, 5)\n441\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(len(df.question.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "question        What is the characteristic feature of the Dyke...\n",
       "context         Left hemisphere and male sex dominance of cere...\n",
       "answer_text                                  cerebral hemiatrophy\n",
       "answer_start                                                  130\n",
       "answer_end                                                    150\n",
       "Name: 240, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "sample_question = df.iloc[240]\n",
    "sample_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_answer(question):\n",
    "    answer_start , answer_end = question[\"answer_start\"], question[\"answer_end\"]\n",
    "    context = question[\"context\"]\n",
    "\n",
    "    return colored(context[:answer_start], \"white\") + \\\n",
    "        colored(context[answer_start:answer_end], \"green\") + \\\n",
    "        colored(context[answer_end:], \"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "What is the characteristic feature of the Dyke-Davidoff-Masson syndrome.\n\n\u001b[37mLeft hemisphere and male sex dominance of cerebral hemiatrophy (Dyke-Davidoff-Masson Syndrome). Although radiological\nfindings of \u001b[0m\u001b[32mcerebral hemiatrophy\u001b[0m\u001b[37m (Dyke-Davidoff-Masson Syndrome) are well known, there is no systematic study\nabout the gender and the affected side in this syndrome. Brain images in 26 patients (mean aged 11) with cerebral hemiatrophy were\nretrospectively reviewed. Nineteen patients (73.5%) were male and seven patients (26.5%) were female. Left hemisphere involvement\nwas seen in 18 patients (69.2%) and right hemisphere involvement was seen in eight patients (30.8%). We conclude that male gender\nand left side involvement are frequent in cerebral hemiatrophy disease.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(sample_question[\"question\"])\n",
    "print()\n",
    "for wrap in textwrap.wrap(color_answer(sample_question), width = 130):\n",
    "    print(wrap)"
   ]
  },
  {
   "source": [
    "### Tokenization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_encoding = tokenizer(\n",
    "    \"Would I rather be feared or loved?\",\n",
    "    \"Easy. Both, I want both.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "sample_encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[5328, 27, 1066, 36, 3, 27625, 42, 1858, 58, 1, 6844, 5, 2867, 6, 27, 241, 321, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "print(sample_encoding[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(sample_encoding[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [\n",
    "    tokenizer.decode(input_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    for input_id in sample_encoding[\"input_ids\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Would I rather be  feared or loved ? </s> Easy . Both , I want both . </s>'"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "\" \".join(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer(\n",
    "    sample_question[\"question\"],\n",
    "    sample_question[\"context\"],\n",
    "    max_length=396,\n",
    "    padding=\"max_length\",\n",
    "    truncation=\"only_second\",\n",
    "    return_attention_mask=True,\n",
    "    add_special_tokens=True,\n",
    "    return_tensors=\"pt\"\n",
    "    )\n",
    "# truncation=\"only_second\" because we do not want to truncate the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'pad_token': '<pad>',\n",
       " 'additional_special_tokens': \"['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']\"}"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('</s>', 1)"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "tokenizer.eos_token, tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'What is the characteristic feature of the Dyke-Davidoff-Masson syndrome.</s> Left hemisphere and male sex dominance of cerebral hemiatrophy (Dyke-Davidoff-Masson Syndrome). Although radiological findings of cerebral hemiatrophy (Dyke-Davidoff-Masson Syndrome) are well known, there is no systematic study about the gender and the affected side in this syndrome. Brain images in 26 patients (mean aged 11) with cerebral hemiatrophy were retrospectively reviewed. Nineteen patients (73.5%) were male and seven patients (26.5%) were female. Left hemisphere involvement was seen in 18 patients (69.2%) and right hemisphere involvement was seen in eight patients (30.8%). We conclude that male gender and left side involvement are frequent in cerebral hemiatrophy disease.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>'"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "tokenizer.decode(encoding[\"input_ids\"].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_encoding = tokenizer(\n",
    "    sample_question[\"answer_text\"],\n",
    "    max_length=32,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    add_special_tokens=True,\n",
    "    return_tensors=\"pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'cerebral hemiatrophy</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>'"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "tokenizer.decode(answer_encoding[\"input_ids\"].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[24387,     3,   107, 11658,    17, 29006,     1,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]])"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "labels = answer_encoding[\"input_ids\"]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[24387,     3,   107, 11658,    17, 29006,     1,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100]])"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "# We need to convert the labels that are ignored or masked to -100\n",
    "labels[labels == 0] = -100\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioQADataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        tokenizer: T5Tokenizer,\n",
    "        source_max_token_len: int = 396,\n",
    "        target_max_token_len: int = 32\n",
    "    ):\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.source_max_token_len = source_max_token_len\n",
    "        self.target_max_token_len = target_max_token_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        data_row = self.data.iloc[index]\n",
    "\n",
    "        source_encoding = tokenizer(\n",
    "            data_row[\"question\"],\n",
    "            data_row[\"context\"],\n",
    "            max_length=self.source_max_token_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=\"only_second\",\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        target_encoding = tokenizer(\n",
    "            data_row[\"answer_text\"],\n",
    "            max_length=self.source_max_token_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels = target_encoding[\"input_ids\"]\n",
    "        labels[labels == 0] = -100\n",
    "\n",
    "        return dict(\n",
    "            question=data_row[\"question\"],\n",
    "            context=data_row[\"context\"],\n",
    "            answer_text=data_row[\"answer_text\"],\n",
    "            input_ids=source_encoding[\"input_ids\"].flatten(),\n",
    "            attention_mask=source_encoding[\"attention_mask\"].flatten(),\n",
    "            labels=labels.flatten()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = BioQADataset(df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "What is the inheritance pattern of Li–Fraumeni syndrome?\nautosomal dominant\ntensor([  363,    19,     8, 28915,  3275,    13,  1414,   104,   371,  6340,\n           35,    23, 12398,    58,     1, 17904,    26,     3,    17,   599])\ntensor([ 1510, 10348,   138, 12613,     1,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100])\n"
     ]
    }
   ],
   "source": [
    "for data in sample_dataset:\n",
    "    print(data[\"question\"])\n",
    "    print(data[\"answer_text\"])\n",
    "    print(data[\"input_ids\"][:20])\n",
    "    print(data[\"labels\"][:20])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((2452, 5), (130, 5))"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.05)\n",
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioQADataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_df: pd.DataFrame,\n",
    "        test_df: pd.DataFrame,\n",
    "        tokenizer: T5Tokenizer,\n",
    "        batch_size: int = 8,\n",
    "        source_max_token_len: int = 396,\n",
    "        target_max_token_len: int = 32\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.source_max_token_len = source_max_token_len\n",
    "        self.target_max_token_len = target_max_token_len\n",
    "\n",
    "    def setup(self):\n",
    "        self.train_dataset = BioQADataset(\n",
    "            self.train_df,\n",
    "            self.tokenizer,\n",
    "            self.source_max_token_len,\n",
    "            self.target_max_token_len\n",
    "        )\n",
    "\n",
    "        self.test_dataset = BioQADataset(\n",
    "            self.test_df,\n",
    "            self.tokenizer,\n",
    "            self.source_max_token_len,\n",
    "            self.target_max_token_len\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=N_WORKERS\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=1,\n",
    "            num_workers=N_WORKERS\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=1,\n",
    "            num_workers=N_WORKERS\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "N_EPOCHS = 6\n",
    "\n",
    "data_module = BioQADataModule(train_df, val_df, tokenizer, batch_size=BATCH_SIZE)\n",
    "data_module.setup()"
   ]
  },
  {
   "source": [
    "# Second video  \n",
    "https://www.youtube.com/watch?t=348&v=r6XY80Z9eSA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at C:/TransformerModels/t5-base-qa-qg-hl were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_FILES, return_dict=True)"
   ]
  },
  {
   "source": [
    "# Translation  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[    0,  1674,  1131,    15,  2221,     6,    92,  2010,     3,   362,\n",
       "         29484,     6,  2278, 12426, 27017,     1]])"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "input_ids = tokenizer(\n",
    "    \"translate English to German: I talk a lot, so I've learned to tune myself out\",\n",
    "    return_tensors=\"pt\"\n",
    ").input_ids\n",
    "\n",
    "generated_ids = model.generate(input_ids=input_ids)\n",
    "generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Ich rede viel, also habe ich gelernt, mich auszuschalten']"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "preds = [\n",
    "    tokenizer.decode(gen_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    for gen_id in generated_ids\n",
    "]\n",
    "\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Ich rede viel, also habe ich gelernt, mich auszuschalten'"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "\" \".join(preds)"
   ]
  },
  {
   "source": [
    "### back to english with google  \n",
    "https://translate.google.com/?sl=auto&tl=en&text=Ich%20rede%20viel%2C%20also%20habe%20ich%20gelernt%2C%20mich%20auszuschalten&op=translate\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Summarization\n",
    "\n",
    "\n",
    "How to generate text: using different decoding methods for language generation with Transformers  \n",
    "https://huggingface.co/blog/how-to-generate"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "summarize: The FDA, an agency within the U.S. Department of Health and Human Services, protects the public health by assuring the safety, effectiveness, and security of human and veterinary drugs, vaccines and other biological products for human use, and medical devices.\n",
    "The agency also is responsible for the safety and security of our nation’s food supply, cosmetics, dietary supplements, products that give off electronic radiation, and for regulating tobacco products.\n",
    "The agency has updated its FDA COVID-19 Response At-A-Glance Summary, which provides a quick look at facts, figures, and highlights on the FDA's response efforts.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'The FDA is responsible for the safety and security of human and veterinary drugs, vaccines and'"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "input_ids = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\"\n",
    ").input_ids\n",
    "\n",
    "generated_ids = model.generate(input_ids=input_ids)\n",
    "\n",
    "preds = [\n",
    "    tokenizer.decode(gen_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    for gen_id in generated_ids\n",
    "]\n",
    "\n",
    "\" \".join(preds)"
   ]
  },
  {
   "source": [
    "# Question answering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(\n",
    " input_ids = encoding[\"input_ids\"],\n",
    " attention_mask=encoding[\"attention_mask\"],\n",
    " labels=labels\n",
    ")"
   ]
  },
  {
   "source": [
    "#### encoding was defined previously:\n",
    "<pre>\n",
    "encoding = tokenizer(\n",
    "    sample_question[\"question\"],\n",
    "    sample_question[\"context\"],\n",
    "    max_length=396,\n",
    "    padding=\"max_length\",\n",
    "    truncation=\"only_second\",\n",
    "    return_attention_mask=True,\n",
    "    add_special_tokens=True,\n",
    "    return_tensors=\"pt\"\n",
    "    )\n",
    "# truncation=\"only_second\" because we do not want to truncate the question\n",
    "</pre>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "T5Config {\n",
       "  \"_name_or_path\": \"C:/TransformerModels/t5-base-qa-qg-hl\",\n",
       "  \"architectures\": [\n",
       "    \"T5ForConditionalGeneration\"\n",
       "  ],\n",
       "  \"d_ff\": 3072,\n",
       "  \"d_kv\": 64,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_start_token_id\": 0,\n",
       "  \"dropout_rate\": 0.1,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"feed_forward_proj\": \"relu\",\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"layer_norm_epsilon\": 1e-06,\n",
       "  \"model_type\": \"t5\",\n",
       "  \"n_positions\": 512,\n",
       "  \"num_decoder_layers\": 12,\n",
       "  \"num_heads\": 12,\n",
       "  \"num_layers\": 12,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"task_specific_params\": {\n",
       "    \"translation_en_to_fr\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 32,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"\"\n",
       "    }\n",
       "  },\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32102\n",
       "}"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 32102])"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "output.logits.shape # see model.config ; 32102 is from vocabulary size; 32 comes from relative_attention_num_buckets; 1 is the batch size, a single example\n",
    "# for each one of the 32102 vocabulary entry we have 32 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(2.3435, grad_fn=<NllLossBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "output.loss"
   ]
  },
  {
   "source": [
    "### Modeling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioQAModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_FILES, return_dict=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None): # labels are optional because they are not supplied when testing\n",
    "        output = self.model(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "            )\n",
    "\n",
    "        return output.loss, output.logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at C:/TransformerModels/t5-base-qa-qg-hl were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BioQAModel()"
   ]
  },
  {
   "source": [
    "# Model Training using our dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Checkpoint callback to save best model found during trainig\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=CHECKPOINT_PATH,\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"best-checkpoint\",\n",
    "    save_top_k=1, #just keep the best one\n",
    "    verbose=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\" # save the one with minimum validation loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: None, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    checkpoint_callback = checkpoint_callback,\n",
    "    max_epochs = N_EPOCHS,\n",
    "    gpus=N_GPUS,\n",
    "    progress_bar_refresh_rate=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Reusing TensorBoard on port 6006 (pid 16668), started 17:47:50 ago. (Use '!kill 16668' to kill it.)"
     },
     "metadata": {}
    }
   ],
   "source": [
    "%tensorboard --logdir ./lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 222 M \n",
      "-----------------------------------------------------\n",
      "222 M     Trainable params\n",
      "0         Non-trainable params\n",
      "222 M     Total params\n",
      "Epoch 0:  14%|█▎        | 60/437 [2:24:33<15:08:16, 144.55s/it, loss=0.47, v_num=0, val_loss=3.11, train_loss=0.547] "
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "source": [
    "# Predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = BioQAModel.load_from_checkpoint(CHECKPOINT_PATH + \"/best-checkpoint.ckpt\")\n",
    "trained_model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question):\n",
    "    source_encoding = tokenizer(\n",
    "        question[\"question\"],\n",
    "        question[\"context\"],\n",
    "        max_length=396,\n",
    "        padding=\"max_length\",\n",
    "        truncation=\"only_second\", # do not truncate question\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    generated_ids = trained_model.model.generate(\n",
    "        input_ids=source_encoding[\"input_ids\"],\n",
    "        attention_mask=source_encoding[\"attention_mask\"],\n",
    "        num_beams=1,\n",
    "        max_length=80,\n",
    "        repetition_penalty=2.5,\n",
    "        length_penalty=1.0,\n",
    "        early_stopping=True,\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "    preds = [\n",
    "        tokenizer.decode(generated_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        for generated_id in generated_ids\n",
    "    ]\n",
    "\n",
    "    return \" \".join(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_question = val_df.iloc[0]\n",
    "print(sample_question[\"question\"])\n",
    "print(sample_question[\"answer_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_answer(sample_question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('T5': conda)",
   "metadata": {
    "interpreter": {
     "hash": "038919695edc8718096e6e2ab8ec781ec9168d3ef58227ffd12720d5a23c1361"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}