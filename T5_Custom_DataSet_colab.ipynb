{
 "cells": [
  {
   "source": [
    "#### T5  \n",
    "https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html (https://youtu.be/r6XY80Z9eSA?t=348)  \n",
    "https://huggingface.co/transformers/model_doc/t5.html  \n",
    "https://arxiv.org/abs/1910.10683  \n",
    "\n",
    "\n",
    "https://www.youtube.com/watch?v=_l2wJb3QPdk  \n",
    "https://www.youtube.com/watch?v=r6XY80Z9eSA  \n",
    "\n",
    "## Tutorial, Part 1\n",
    "https://www.youtube.com/watch?v=_l2wJb3QPdk\n",
    "\n",
    "In google colab change runtime type to GPU"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet transformers==4.1.1\n",
    "# !pip install --quiet pytorch-lightning==1.1.1\n",
    "!pip install --quiet torchtext==0.8.0 torch==1.7.1 pytorch-lightning==1.2.2\n",
    "!pip install --quiet tokenizers==0.9.4\n",
    "!pip install --quiet sentencepiece==0.1.94\n",
    "!pip install --quiet pandas  \n",
    "!pip install --quiet sklearn\n",
    "!pip install --quiet keras\n",
    "!pip install --quiet tensorflow\n",
    "!pip install --quiet termcolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as py\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from termcolor import colored\n",
    "import textwrap\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "\n",
    "# model files are downloaded from https://huggingface.co/valhalla/t5-base-qa-qg-hl/tree/main\n",
    "# if Internet access is available just use\n",
    "# MODEL_FILES = \"t5-base\"\n",
    "# instead of path to model files\n",
    "\n",
    "#from sys import platform\n",
    "#if \"linux\" in platform.lower():\n",
    "#    MODEL_FILES = \"/home/myuser/TransformerModels/t5-base-qa-qg-hl\"\n",
    "#    CHECKPOINT_PATH=\"/home/myuserTransformerModels/_CheckPoints\"\n",
    "#else:\n",
    "#    MODEL_FILES = \"C:/TransformerModels/t5-base-qa-qg-hl\"\n",
    "#    CHECKPOINT_PATH=\"C:/TransformerModels/_CheckPoints\"\n",
    "\n",
    "MODEL_FILES = \"t5-base\"\n",
    "CHECKPOINT_PATH=\"./CheckPoints\"\n",
    "\n",
    "\n",
    "N_GPUS = 1 # Change here if you have GPUs\n",
    "N_WORKERS = 4 # 4 in the tutorial. 0 if running on windows without GPU...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = AutoModelWithLMHead.from_pretrained(\"deep-learning-analytics/triviaqa-t5-base\")\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/dmis-lab/biobert#datasets\n",
    "!gdown --id 19ft5q44W4SuptJgTwR84xZjsHg1jvjSZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q QA.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(\"BioASQ/BioASQ-train-factoid-4b.json\").open() as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = data[\"data\"][0][\"paragraphs\"]\n",
    "questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_questions_and_answers(factoid_path: Path):\n",
    "    with factoid_path.open() as json_file:\n",
    "        data = json.load(json_file)\n",
    "        \n",
    "    questions = data[\"data\"][0][\"paragraphs\"]\n",
    "    data_rows = []\n",
    "    for question in questions:\n",
    "        context = question['context']\n",
    "        for question_and_answers in question['qas']:\n",
    "            question = question_and_answers[\"question\"]\n",
    "            answers = question_and_answers[\"answers\"]\n",
    "            \n",
    "        for answer in answers:\n",
    "            answer_text = answer[\"text\"]\n",
    "            answer_start = answer[\"answer_start\"]\n",
    "            answer_end = answer_start + len(answer_text)\n",
    "            \n",
    "            data_rows.append({\n",
    "                \"question\":question,\n",
    "                \"context\":context,\n",
    "                \"answer_text\": answer_text,\n",
    "                \"answer_start\":answer_start,\n",
    "                \"answer_end\":answer_end\n",
    "            })\n",
    "            \n",
    "    return pd.DataFrame(data_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_questions_and_answers(Path(\"BioASQ/BioASQ-train-factoid-4b.json\")).head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factoid_paths = sorted(list(Path(\"BioASQ/\").glob(\"BioASQ-train-*\")))\n",
    "factoid_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for factoid_path in factoid_paths:\n",
    "    dfs.append(extract_questions_and_answers(factoid_path))\n",
    "    \n",
    "df = pd.concat(dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df.question.unique()))\n",
    "print(len(df.answer_text.unique()))\n",
    "print(len(df.context.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "source": [
    "## The duplicates removal in the following cells is done and explained in the second video"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(len(df.question.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=[\"context\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(len(df.question.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_question = df.iloc[240]\n",
    "sample_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_answer(question):\n",
    "    answer_start , answer_end = question[\"answer_start\"], question[\"answer_end\"]\n",
    "    context = question[\"context\"]\n",
    "\n",
    "    return colored(context[:answer_start], \"white\") + \\\n",
    "        colored(context[answer_start:answer_end], \"green\") + \\\n",
    "        colored(context[answer_end:], \"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_question[\"question\"])\n",
    "print()\n",
    "for wrap in textwrap.wrap(color_answer(sample_question), width = 130):\n",
    "    print(wrap)"
   ]
  },
  {
   "source": [
    "### Tokenization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_encoding = tokenizer(\n",
    "    \"Would I rather be feared or loved?\",\n",
    "    \"Easy. Both, I want both.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_encoding[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_encoding[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [\n",
    "    tokenizer.decode(input_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    for input_id in sample_encoding[\"input_ids\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer(\n",
    "    sample_question[\"question\"],\n",
    "    sample_question[\"context\"],\n",
    "    max_length=396,\n",
    "    padding=\"max_length\",\n",
    "    truncation=\"only_second\",\n",
    "    return_attention_mask=True,\n",
    "    add_special_tokens=True,\n",
    "    return_tensors=\"pt\"\n",
    "    )\n",
    "# truncation=\"only_second\" because we do not want to truncate the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.eos_token, tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(encoding[\"input_ids\"].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_encoding = tokenizer(\n",
    "    sample_question[\"answer_text\"],\n",
    "    max_length=32,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    add_special_tokens=True,\n",
    "    return_tensors=\"pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(answer_encoding[\"input_ids\"].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = answer_encoding[\"input_ids\"]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to convert the labels that are ignored or masked to -100\n",
    "labels[labels == 0] = -100\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioQADataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        tokenizer: T5Tokenizer,\n",
    "        source_max_token_len: int = 396,\n",
    "        target_max_token_len: int = 32\n",
    "    ):\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.source_max_token_len = source_max_token_len\n",
    "        self.target_max_token_len = target_max_token_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        data_row = self.data.iloc[index]\n",
    "\n",
    "        source_encoding = tokenizer(\n",
    "            data_row[\"question\"],\n",
    "            data_row[\"context\"],\n",
    "            max_length=self.source_max_token_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=\"only_second\",\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        target_encoding = tokenizer(\n",
    "            data_row[\"answer_text\"],\n",
    "            max_length=self.source_max_token_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels = target_encoding[\"input_ids\"]\n",
    "        labels[labels == 0] = -100\n",
    "\n",
    "        return dict(\n",
    "            question=data_row[\"question\"],\n",
    "            context=data_row[\"context\"],\n",
    "            answer_text=data_row[\"answer_text\"],\n",
    "            input_ids=source_encoding[\"input_ids\"].flatten(),\n",
    "            attention_mask=source_encoding[\"attention_mask\"].flatten(),\n",
    "            labels=labels.flatten()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = BioQADataset(df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in sample_dataset:\n",
    "    print(data[\"question\"])\n",
    "    print(data[\"answer_text\"])\n",
    "    print(data[\"input_ids\"][:20])\n",
    "    print(data[\"labels\"][:20])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.05)\n",
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioQADataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_df: pd.DataFrame,\n",
    "        test_df: pd.DataFrame,\n",
    "        tokenizer: T5Tokenizer,\n",
    "        batch_size: int = 8,\n",
    "        source_max_token_len: int = 396,\n",
    "        target_max_token_len: int = 32\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.source_max_token_len = source_max_token_len\n",
    "        self.target_max_token_len = target_max_token_len\n",
    "\n",
    "    def setup(self):\n",
    "        self.train_dataset = BioQADataset(\n",
    "            self.train_df,\n",
    "            self.tokenizer,\n",
    "            self.source_max_token_len,\n",
    "            self.target_max_token_len\n",
    "        )\n",
    "\n",
    "        self.test_dataset = BioQADataset(\n",
    "            self.test_df,\n",
    "            self.tokenizer,\n",
    "            self.source_max_token_len,\n",
    "            self.target_max_token_len\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=N_WORKERS\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=1,\n",
    "            num_workers=N_WORKERS\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=1,\n",
    "            num_workers=N_WORKERS\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "N_EPOCHS = 6\n",
    "\n",
    "data_module = BioQADataModule(train_df, val_df, tokenizer, batch_size=BATCH_SIZE)\n",
    "data_module.setup()"
   ]
  },
  {
   "source": [
    "# Second video  \n",
    "https://www.youtube.com/watch?t=348&v=r6XY80Z9eSA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_FILES, return_dict=True)"
   ]
  },
  {
   "source": [
    "# Translation  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\n",
    "    \"translate English to German: I talk a lot, so I've learned to tune myself out\",\n",
    "    return_tensors=\"pt\"\n",
    ").input_ids\n",
    "\n",
    "generated_ids = model.generate(input_ids=input_ids)\n",
    "generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [\n",
    "    tokenizer.decode(gen_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    for gen_id in generated_ids\n",
    "]\n",
    "\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(preds)"
   ]
  },
  {
   "source": [
    "### back to english with google  \n",
    "https://translate.google.com/?sl=auto&tl=en&text=Ich%20rede%20viel%2C%20also%20habe%20ich%20gelernt%2C%20mich%20auszuschalten&op=translate\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Summarization\n",
    "\n",
    "\n",
    "How to generate text: using different decoding methods for language generation with Transformers  \n",
    "https://huggingface.co/blog/how-to-generate"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "summarize: The FDA, an agency within the U.S. Department of Health and Human Services, protects the public health by assuring the safety, effectiveness, and security of human and veterinary drugs, vaccines and other biological products for human use, and medical devices.\n",
    "The agency also is responsible for the safety and security of our nationâ€™s food supply, cosmetics, dietary supplements, products that give off electronic radiation, and for regulating tobacco products.\n",
    "The agency has updated its FDA COVID-19 Response At-A-Glance Summary, which provides a quick look at facts, figures, and highlights on the FDA's response efforts.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\"\n",
    ").input_ids\n",
    "\n",
    "generated_ids = model.generate(input_ids=input_ids)\n",
    "\n",
    "preds = [\n",
    "    tokenizer.decode(gen_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    for gen_id in generated_ids\n",
    "]\n",
    "\n",
    "\" \".join(preds)"
   ]
  },
  {
   "source": [
    "# Question answering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(\n",
    " input_ids = encoding[\"input_ids\"],\n",
    " attention_mask=encoding[\"attention_mask\"],\n",
    " labels=labels\n",
    ")"
   ]
  },
  {
   "source": [
    "#### encoding was defined previously:\n",
    "<pre>\n",
    "encoding = tokenizer(\n",
    "    sample_question[\"question\"],\n",
    "    sample_question[\"context\"],\n",
    "    max_length=396,\n",
    "    padding=\"max_length\",\n",
    "    truncation=\"only_second\",\n",
    "    return_attention_mask=True,\n",
    "    add_special_tokens=True,\n",
    "    return_tensors=\"pt\"\n",
    "    )\n",
    "# truncation=\"only_second\" because we do not want to truncate the question\n",
    "</pre>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.logits.shape # see model.config ; 32102 is from vocabulary size; 32 comes from relative_attention_num_buckets; 1 is the batch size, a single example\n",
    "# for each one of the 32102 vocabulary entry we have 32 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.loss"
   ]
  },
  {
   "source": [
    "### Modeling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioQAModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_FILES, return_dict=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None): # labels are optional because they are not supplied when testing\n",
    "        output = self.model(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "            )\n",
    "\n",
    "        return output.loss, output.logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BioQAModel()"
   ]
  },
  {
   "source": [
    "# Model Training using our dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Checkpoint callback to save best model found during trainig\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=CHECKPOINT_PATH,\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"best-checkpoint\",\n",
    "    save_top_k=1, #just keep the best one\n",
    "    verbose=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\" # save the one with minimum validation loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    checkpoint_callback = checkpoint_callback,\n",
    "    max_epochs = N_EPOCHS,\n",
    "    gpus=N_GPUS,\n",
    "    progress_bar_refresh_rate=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "source": [
    "# Predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = BioQAModel.load_from_checkpoint(CHECKPOINT_PATH + \"/best-checkpoint.ckpt\")\n",
    "trained_model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question):\n",
    "    source_encoding = tokenizer(\n",
    "        question[\"question\"],\n",
    "        question[\"context\"],\n",
    "        max_length=396,\n",
    "        padding=\"max_length\",\n",
    "        truncation=\"only_second\", # do not truncate question\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    generated_ids = trained_model.model.generate(\n",
    "        input_ids=source_encoding[\"input_ids\"],\n",
    "        attention_mask=source_encoding[\"attention_mask\"],\n",
    "        num_beams=1,\n",
    "        max_length=80,\n",
    "        repetition_penalty=2.5,\n",
    "        length_penalty=1.0,\n",
    "        early_stopping=True,\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "    preds = [\n",
    "        tokenizer.decode(generated_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        for generated_id in generated_ids\n",
    "    ]\n",
    "\n",
    "    return \" \".join(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_question = val_df.iloc[0]\n",
    "print(sample_question[\"question\"])\n",
    "print(sample_question[\"answer_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_answer(sample_question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('T5': conda)",
   "metadata": {
    "interpreter": {
     "hash": "038919695edc8718096e6e2ab8ec781ec9168d3ef58227ffd12720d5a23c1361"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
